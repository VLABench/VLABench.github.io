<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks">
  <meta name="keywords" content="Language-Conditioned Robotic Manipulation, Benchmark, Dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title style="letter-spacing: 0.1em; font-variant: small-caps;">VLABench</title>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R2ZLMKHR2E"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-R2ZLMKHR2E');
</script>

  <script>
    function updateInteractive() {
      var task = document.getElementById("interactive-menu").value;

      console.log("interactive", task)
      // Update Video
      var video_base = document.getElementById("video-base");
      video_base.src = "./media/primitive_task/" + 
                  task + "/" +
                  "base.mp4"
      video_base.play();

      var video_spatial = document.getElementById("video-spatial");
      video_spatial.src = "./media/primitive_task/" + 
                  task + "/" +
                  "spatial.mp4"
      video_spatial.play();

      var video_commonsense = document.getElementById("video-commonsense");
      video_commonsense.src = "./media/primitive_task/" + 
                  task + "/" +
                  "commonsense.mp4"
      video_commonsense.play();

      var video_semantic = document.getElementById("video-semantic");
      video_semantic.src = "./media/primitive_task/" + 
                  task + "/" +
                  "semantic.mp4"
      video_semantic.play();

    // Update Text Description
    fetch("./media/primitive_task/" + task + "/" + "base.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load base text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("base-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("base-text").innerText = "Error loading description.";
    });

    fetch("./media/primitive_task/" + task + "/" + "spatial.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load spatial text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("spatial-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("spatial-text").innerText = "Error loading description.";
    });

    fetch("./media/primitive_task/" + task + "/" + "commonsense.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load commonsense text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("commonsense-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("commonsense-text").innerText = "Error loading description.";
    });

    fetch("./media/primitive_task/" + task + "/" + "semantic.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load semantic text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("semantic-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("semantic-text").innerText = "Error loading description.";
    });
    }


  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static//js/leaderboard.js" type="module"></script>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a class="author-name" href="https://shiduo-zhang.github.io/">Shiduo Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Zhe Xu<sup>1</sup>,
            </span>
            <span class="author-block">
              Peiju Liu<sup>1*</sup>,
            </span>
            <span class="author-block">
              Xiaopeng Yu<sup>1*</sup>,
            </span>
            <span class="author-block">
              Yuan Li<sup>1</sup>,
            </span>
            <span class="author-block">
              Qinghui Gao<sup>1</sup>,
            </span>
        </br>
            <span class="author-block">
              Zhaoye Fei<sup>1</sup>,
            </span>
            <span class="author-block">
              Zhangyue Yin<sup>1</sup>,
            </span>
            <span class="author-block">
              Zuxuan Wu<sup>1</sup>,
            </span>
            <span class="author-block">
              Yu-Gang Jiang<sup>1</sup>,
              </span>
              <span class="author-block">
                <a class="author-name" href="https://xpqiu.github.io/en.html">Xipeng Qiu</a><sup>1</sup>
              </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>School of Computer Science and Technology, Fudan University. </span>
            </br>
            <span class="author-block"><sup>*</sup>Equal Contribution. </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="./media/VLAbench_arxiv_v1.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2412.18194"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>Arxiv</span>
              </a>
            </span>
            <!-- <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" width="50"> -->
            <!-- Video Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://github.com"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video(On the way)</span>
              </a>
            </span> -->

            <!-- Hugging face Link -->
            <span class="link-block">
              <a target="_blank" href="https://huggingface.co/VLABench"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                  alt="Hugging Face" width="24" style="vertical-align: middle;">
                </span>
                <span>Dataset</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/OpenMOSS/VLABench"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            
            <!-- Twitter Link. -->
            <span class="link-block">
              <a target="_blank" href="https://x.com/Joey_zh_/status/1872021219597578734"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-twitter"></i>
                </span>
                <span>Twitter</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-bottom: 1em;">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks. 
          </p>
        </div>
      </div>
    </div>
  </div>

</section>

<!-- overview figure -->
<div class="hero-body">
  <div class="container is-max-widescreen">
    <div class="container has-text-centered">
      <img src="./media/task_static/main_page.gif" alt="gif">
    </div>
  </div>
</div>


<section class="section">
  <div class="container is-max-widescreen">
    <!-- Section Title -->
    <div class="has-text-centered" style="margin-bottom: 2em;">
      <h1 class="title is-3">VLABench</h1>
      <h2 class="title is-4">New Definition for LCM Tasks Suitable for Foundation Models</h2>
      <h2 class="subtitle is-5">——What Abilities Should a True VLA Have?</h2>
    </div>

    <!-- Content with Image and Text -->
    <div class="columns is-vcentered">
      <!-- Left Column: Image Placeholder
      <div class="column is-half has-text-centered">
        <div class="box" style="height: 300px; background-color: #f5f5f5; border: 1px dashed #ccc;">
          <p class="is-size-5 has-text-grey">Image Placeholder</p>
        </div>
      </div> -->

      <!-- Right Column: Text Placeholder -->
     
        <div class="content">
          <p class="is-size-5">
            From the perspective of “intelligence”, it is divided into six capability dimensions:
          </p>
          <ul>
            <li><span style="font-weight: bolder;">Mesh & Texture Understanding.</span>
              It should be able to recognize irregular and uniquely shaped meshes as well as diversified textures with rich semantic information. This involves basic open-vocabulary object recognition, OCR capabilities and etc.
            </li>
            <li><span style="font-weight: bolder;">Spatial Understanding.</span>
              It should possess basic spatial perception abilities, enabling accurate judgment of the <span style="font-weight: bolder;">relative positions</span> of objects in an image,<span style="font-weight: bolder;">spatial constraints</span> between different objects, and even direct <span style="font-weight: bolder;">distance estimation</span>.
            </li>
            <li><span style="font-weight: bolder;">Common Sense & World Knowledge Transfer.</span>
              It should acquire world knowledge and common sense from large-scale pretraining and apply such priors to corresponding tasks. For example, associating visual information with world knowledge to align it with user requirements.
            </li>
            <li><span style="font-weight: bolder;">Semantic Instruction Understanding.</span>
              It should retain strong language comprehension abilities, enabling it to extract user needs from natural interactions or understand the implicit goals of a task, and then execute dynamic action sequences. Instead of template instructions like “pick A and then place it to B”.
            </li>
            <li><span style="font-weight: bolder;">Physical Laws Understanding.</span>
              It should understand the principles of the physical world, such as friction, gravity, acceleration, and even fundamental physical concepts like the lever principle.
            </li>
            <li><span style="font-weight: bolder;">Long-Horizon Reasoning.</span> 
              Reasoning here primarily refers to the ability to plan for long-horizon, multi-step tasks, where logical correlations between multiple action steps are required. Broader reasoning encompasses several of the aforementioned abilities, such as semantic inference, the incorporation of world knowledge, and alignment between vision and task objectives. However, in this context, we focus solely on the former.
            </li>
          </ul>
        </div>
    </div>
  </div>
</section>

<!-- Primitive Display -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="container has-text-centered">
      <h1 class="title is-4" style="margin-bottom: 1em;">100 Tasks in VLABench</h1>
      <h2 class="subtitle is-5" style="margin-bottom: 1em;">——What Tasks Should a True VLA Do?</h2>
    </div>
    <div>
      <p>
        VLABench divides tasks into two categories: Primitive and Composite.
      </p>
      <ul>
        <li><span style="font-weight: bolder;">Primitive Tasks:</span> 60 tasks that require only one or two dimensions abilities and few skill combinations.</li>
        <li><span style="font-weight: bolder;">Composite Tasks:</span> 40 tasks that require multi-step reasoning and long-horizon planning, involving more skills and abilities.</li>
      </ul>
      <scan style="line-height: 1.5; font-style: italic;">The 100 task categories are just a starting point. E.g. New tasks can be created by arbitrarily combining the above-mentioned abilities and skills.</scan>
    </div>
    <div class="rows">
    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <!-- Interactive Visualization -->
        <div class="container has-text-centered">
          <h2 class="title is-5" style="margin-bottom: 1em;margin-top: 1em;">Example of Primitive Tasks</h2>
        </div>

        <div class="columns is-vcentered">
          <div class="column has-text-centered">

            Visualization for   
            <div class="select is-small is-rounded">     
              <select id="interactive-menu" onchange="updateInteractive()">
              <option value="select_fruit" selected="selected">"Select Fruit Series"</option>
              <option value="select_toy">"Select Toy Series"</option>
              <option value="select_chemistry_tube">"Select Chemistry Tube Series"</option>
              <option value="add_condiment">"Add Condiment Series"</option>
              <option value="select_book">"Select Book Series"</option>
              <option value="select_painting">"Select Painting Series"</option>
              <option value="select_drink">"Select Drink Series"</option>
              <option value="insert_flower">"Insert Flower Series"</option>
              <!-- <option value=""> -->
              </select>
            </div>
          </div>

        </div>
        <div class="columns is-vcentered">
            <div class="column is-one-fourth">
              <div class="instruction-panel ">
              <p id="base-text">Pick the banana into the plate.</p>
              </div>
              <video id="video-base" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                <source src="./media/primitive_task/select_fruit/base.mp4" type="video/mp4">
              </video>
              <!-- ability and title here -->
              <p class="text-bottom-title">Mesh & Texture</p>
            </div>
            <div class="column is-one-fourth">
                <div class="instruction-panel">
                <p id="spatial-text">Pick the pear outside into the plate.</p>
                </div>
                <video id="video-spatial" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                    <source src="./media/primitive_task/select_fruit/spatial.mp4" type="video/mp4">
                </video>
                <p class="text-bottom-title">Spatial</p>
            </div>
            <div class="column is-one-fourth">
                <div class="instruction-panel ">
                  <p id="commonsense-text">Pick the fruit with Heat-clearing character into the plate.</p>
                </div>
                <video id="video-commonsense" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                    <source src="./media/primitive_task/select_fruit/commonsense.mp4" type="video/mp4">
                </video>
                <p class="text-bottom-title">Common Sense & World knowledge</p>
            </div>
            <div class="column is-one-fourth">
                <div class="instruction-panel ">
                  <p id="semantic-text">One apple one day, doctor keey away! I want one please.</p>
                </div>
                <video id="video-semantic" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                    <source src="./media/primitive_task/select_fruit/semantic.mp4" type="video/mp4">
                </video>
                <p class="text-bottom-title">Semantic</p>
            </div>
          </div>

        </div>

        </div>
    </div>
  </div>
</section>

<!-- Tasks gallery -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-5" style="margin-bottom: 1em;">Example of Composite Tasks</h2>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <!-- <div class="columns is-multiline is-centered"> -->
        <div class="item">
          <div class="instruction-panel ">
            <p>Instruction: Could you sort the books on the shelf starting with the most recent publication year and ending with the oldest?</p>
          </div>
          <video class="galary-video" id="book_rearrange" poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/book_rearrange/book_rearrange.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Book Rearrange</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Could you help sort the billiard balls into baskets in front of you? We need them organized for the upcoming tournament.</p>
          </div>
          <video id="cluster_billiards" poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cluster_billiards/cluster_billiards.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Cluser Billiards</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Could you help sort out these books on the shelf so the library looks tidy?</p>
          </div>
           <video id="cluster_book" poster="" autoplay muted loop height="100%" controlsList="nodownload">
             <source src="./media/composite_task/cluster_book/cluster_book.mp4"
                     type="video/mp4">
           </video>
           <p class="text-bottom-task-name">Cluser Book</p>
         </div>
         <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Hi! I’m trying to conduct a chemistry experiment. Could you help me prepare Pb(OH)₂ ？</p>
          </div>
          <video id="take_chemistry_experiment" poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/take_chemistry_experiment/take_chemistry_experiment.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Take Chemistry Experiment</p>
         </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Would you mind sorting out the drinks to help keep the bar area tidy and efficient for service?</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cluster_drink/cluster_drink.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Cluser Drink</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Could you sort out the toys in front of you so we can donate the right ones to the local charity?</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cluster_toy/cluster_toy.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Cluser Toy</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Prepare the carrots and corn for a refreshing vegetable salad.</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cook_dishes/cook_dishes.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Cook Dishes</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: I can't seem to find the spirit, could you please find it for me?</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/find_unseen_object/find_unseen_object.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Find Unseen Object</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Hey, I’m feeling a bit sluggish this afternoon; can you make me a coffee with some sugar to perk me up?</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/get_coffee/get_coffee_with_sugar.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Get Latte Coffee</p>
        </div>
       <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Please hang the 武州玉川 on the wall safely and steadily.</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/hammer_nail_and_hang_picture/demo_2_success_True.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Hammer Nail and Hang Picture</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Hey there! Could you please heat up the tray of food for me? Thanks a bunch!</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/heat_food/heat_food.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Heat Proper Food</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Please give the answer of the following question by rearrange the number cube in placemat_seen:Cynthia eats one serving of ice cream every night.  She buys cartons of ice cream with 15 servings of ice cream per carton at a cost of $4.00 per carton.  After 60 days, how much will she spend on ice cream?</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/math_game/math_game.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Play Math Game</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Hey, could you set the table? We're having some friends over for dinner tonight, and the dish is steak.</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/set_dining_table/set_dining_table.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Set Dining Table</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Time for some textile history! Make sure the desk is tidy, and fire up the computer so I can delve into "cotton_the_fabric_that_made_the_modern_world".</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/set_study_table/set_study_table.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Set Study Table</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Hey! it's your turn in Texas Hold'em. Please select the highest-ranking hand available and place them onto placemat.</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/texas_holdem/texas_holdem.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Play Texas Hold'em</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: The cards are dealt, and it's your turn. Analyze and pick the hand with the highest probability of winning. (1 or 2 pokers face down)</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/texas_holdem/texas_holdem_explore.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Explore and Play Texas Hold'em</p>
        </div>
        <div class="item">
          <div class="instruction-panel">
            <p>Instruction: Take me the pear please.</p>
          </div>
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/use_seesaw/use_seesaw.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Simple Seesaw Usage</p>
        </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Evaluation Description -->
<section class="section">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">Evaluation</h2>
      </div>
      <p>The evaluation method of VLABench includes both interactive and non-interactive approaches.
      <br><strong>Interactive:</strong> mainly for evaluation of VLA policies and workflows utilizing VLM/LLM. The policies should step in the environment under specific tasks to compute the progress score and statics success rate.
      <br><strong>Non-interactive:</strong> mainly for evalution of VLMs. The VLMs should generate the action sequence in the format of our skill sequence based on the instruction and the segmented scene images. The generated action sequence will be computed a overall score by matching the DAG between generated and groundtruth. The framework is as the figure below.
      </p>
      <img src="./media/task_static/eval.gif" alt="Example GIF"/>
    </div>
  </div>
</section>

<!-- Leaderboard -->
<section>
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">Experiment Result</h2>
        <h3 class="subtitle is-5" style="margin-bottom: 1em;">——Latest Leaderboad Comming Soon</h3>
      </div>
      <p style="margin-bottom: 1em;">
        VLABench provides standard evaluations for three types of methods: Vision-Language-Action Models, workflows utilizing VLM/LLM, and Vision-Language Models.
        In preview version, We present the <b>early experimental results to facilitate further analysis</b>. The complete leaderboard will be released soon. 
      </p>
      
      
      <!-- vla result -->
      <div class="container has-text-centered">
        <h3 class="title is-4" style="margin-bottom: 1em">Leaderboard of policies(mainly VLAs)</h3>
      </div>
      <p>
        The experimental design related to VLA revolves around the following questions:
        <ul>
          <li><strong>Q1:</strong> Do pre-trained VLAs exhibit stronger general abilities with unseen categories of objects?</li>
          <li><strong>Q2:</strong> Can pre-trained VLAs transfer their general knowledge and behavioral abilities to similar but unseen tasks?</li>
          <li><strong>Q3:</strong> Can pre-trained VLAs understand natural user interactions and implicit goal requirements?</li>
          <li><strong>Q4:</strong> Do pre-trained VLAs have the potential to transfer their world knowledge to related tasks?</li>
          <li><strong>Q5:</strong> Can existing VLA architectures accurately support the completion of long-horizon tasks?</li>
        </ul>
      </p>
      
      <img src="./media/task_static/Figure_VLA_result.png" alt="static image">
      <p>The current VLAs have not demonstrated the expected capabilities, particularly in terms of the intelligence derived from pretraining, as they struggle with tasks involving generalization, skill transfer, and long-horizon planning.
        Drawing an analogy to the development trajectory of large language models, the present state of <span style="color: red;font-weight: bold;">VLAs is still far from reaching a level comparable to GPT-2</span>.
      </p>
      <br>
      <!-- <div class="tab-content" id="myTabContent">
        <div class="tab-pane fade show active" id="benchmark-table-content" role="tabpanel"
            aria-labelledby="benchmark-table-content">
            <div id="behavior-benchmark-main-table"></div>
        </div>
        <div class="tab-pane fade" id="eurus-code-table-content" role="tabpanel"
            aria-labelledby="eurus-code-table-content">
            <div id="virtualhome-benchmark-main-table"></div>
        </div>
    </div> -->
      <!-- workflow result -->
      <div class="container has-text-centered">
        <h3 class="title is-4" style="margin-bottom: 1em">Leaderboard of workflows</h3>
      </div>
      <div class="container">
        <div class="columns is-vcentered">
          <!-- Image Column -->
          <div class="column is-half">
            <img class="image" src="./media/task_static/Figure_Agent_result.png" alt="Sample Image">
          </div>
      
          <!-- Text Column -->
          <div class="column is-half">
            <p>In fact, these so-called zero-shot manipulation workflows are designed for specific types of tasks. When the task scenarios or capability requirements exceed their original design, these methods are less effective.
              <br><br>
              <ul>
                <li><strong>Bottleneck Effect in Submodules</strong>
                  Workflow’s hierarchical and modular design results in a bottleneck effect. 
                  This is particularly evident in the actuator module. E.g. Errors in predicting grasp poses, as well as behaviors that do not satisfy spatial constraints.
                  As well as perception module. E.g. The inability to recognize complex objects leads to failures in the perception stage.
                </li>
                <li><strong>Error in Module Connections</strong>
                  The requirement for specific input-output formats between modules leads to information loss or mismatch. Especially the unexpected behavior of the LLM or VLM. 
                  E.g. The language model did not output the expected rotation matrix, or generated incorrect code that resulted in execution failure.
                </li>
              </ul>
            </p>
          </div>
        </div>
      </div>

      <!-- vlm result -->
      <div class="container has-text-centered">
        <h3 class="title is-4" style="margin-bottom: 1em;">Leaderboard of VLMs</h3>
      </div>
      <div class="container">
        <div class="columns is-vcentered">    
          <!-- Text Column -->
          <div class="column is-half">
            <p>Without considering errors caused by action execution, we evaluated the capabilities of current leading language model series in embodied scenarios using action sequence matching to calculate scores. It was found that these state-of-the-art multimodal models <span style="color: red;font-weight: bold;">do not perform as well as expected in embodied scenarios</span>.
            <br><br>
              <ul>
              <li><strong>Overall Failing Grades</strong>
                Compared to VLAs, these models retain more high-level intelligence. Since they are evaluated primarily from the perception and decision-making aspects, their evaluation scores are relatively higher. Qwen-VL-7B and GPT-4v demonstrated relatively decent performance, but none of VLMs achieved a passing score.
              </li>
              <li><strong>Pour Planning Ability</strong>
                All models performed poorly in task reasoning and planning. Only 4o achieved a relatively balanced score in reasoning.
              </li>
            </ul>
            <br>Please refer to our paper for more detailed analysis.
            </p>
          </div>
          <!-- Image Column -->
          <div class="column is-half">
            <img class="image" src="./media/task_static/Figure_VLM_result.png" alt="Sample Image">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section>
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">Key Insight</h2>
      </div>
    </div>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{zhang2024vlabench,
        title={VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks}, 
        author={Shiduo Zhang and Zhe Xu and Peiju Liu and Xiaopeng Yu and Yuan Li and Qinghui Gao and Zhaoye Fei and Zhangyue Yin and Zuxuan Wu and Yu-Gang Jiang and Xipeng Qiu},
        year={2024},
        eprint={2412.18194},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2412.18194}, 
  }
    </code></pre>
  </div>
</section>
  
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/voxposer/voxposer.github.io">VoxPoser</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>